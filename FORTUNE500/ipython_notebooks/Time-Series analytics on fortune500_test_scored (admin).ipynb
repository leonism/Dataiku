{
  "metadata": {
    "analyzedDataset": "fortune500_test_scored",
    "creator": "admin",
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.9"
    },
    "tags": [],
    "customFields": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Time-Series analytics on fortune500_test_scored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we are going to analyse temporal data.\n",
        "\n",
        "\n",
        "We\u0027ll perform the following analysis:\n",
        "* Interpolating and smoothing of the data. If your data is irregular, it will be transformed to regular data. It can then be \"smoothed\" to get it to a lower granularity.\n",
        "* Various plotting: we\u0027ll plot individual time series on various scales\n",
        "* Aggregations on rolling windows, aka low-pass filters\n",
        "* Seasonality decomposition, to separate the long-term trends from the short-term variations\n",
        "\n",
        "Important note: this notebook does not include forecasting of time series. Please use the \"Time series forecasting\" notebook.\n",
        "\n",
        "Navigate between sections\n",
        "\n",
        "* [Setup and loading the data](#setup)\n",
        "* [Interpolation and smoothing](#interpolation)\n",
        "* [Raw plots](#plots)\n",
        "* [Rolling windows](#window)\n",
        "* [Seasonality decomposition](#seasonality)\n",
        "\n",
        "\u003ccenter\u003e\u003cstrong\u003eSelect Cell \u003e Run All to execute the whole analysis\u003c/strong\u003e\u003c/center\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and dataset loading \u003ca id\u003d\"setup\" /\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First of all, let\u0027s load the libraries that we\u0027ll use"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "%pylab inline\n",
        "import dataiku                                          # Access to Dataiku datasets\n",
        "import pandas as pd, numpy as np                        # Data manipulation \n",
        "import seaborn as sns                                   # Graphing\n",
        "from sklearn.preprocessing import MinMaxScaler          # Rescale utility\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose # For signal decomposition\n",
        "import warnings                                         # Disable some warnings\n",
        "warnings.filterwarnings(\"ignore\",category\u003dDeprecationWarning)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first thing we do is now to load the dataset\n",
        "\n",
        "Since analyzing time series requires to have the data in memory, we are only going to load a sample of the data. Modify the following cell to change the size of the sample."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "mydataset \u003d dataiku.Dataset(\"fortune500_test_scored\")\n",
        "\n",
        "# Load the first 100\u0027000 lines.\n",
        "# You can also load random samples, limit yourself to some columns, or only load\n",
        "# data matching some filters.\n",
        "#\n",
        "# Please refer to the Dataiku Python API documentation for more information\n",
        "df \u003d mydataset.get_dataframe(limit \u003d 100000)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get the columns"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "numerical_columns \u003d list(df.select_dtypes(include\u003d[np.number]).columns)\n",
        "categorical_columns \u003d list(df.select_dtypes(include\u003d[object]).columns)\n",
        "date_columns \u003d list(df.select_dtypes(include\u003d[\u0027\u003cM8[ns]\u0027]).columns)\n",
        "\n",
        "# Print a quick summary of what we just loaded\n",
        "print \"Loaded dataset\"\n",
        "print \"   Rows: %s\" % df.shape[0]\n",
        "print \"   Columns: %s (%s num, %s cat, %s date)\" % (df.shape[1], \n",
        "                                                    len(numerical_columns), len(categorical_columns),\n",
        "                                                    len(date_columns))\n",
        "if not len(date_columns):\n",
        "    print \"No date columns were found, this is an issue. Try to parse your dates before using this notebook!\""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set the time dimension as index\n",
        "\n",
        "All of our analysis will be based on time, so we set the first date as the index of our dataframe"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "time_column \u003d df[date_columns[0]]\n",
        "# We assume the time to be the first date column, uncomment this line to override\n",
        "# time_column \u003d \"my_parsed_date\"\n",
        "\n",
        "df.set_index(time_column, inplace\u003dTrue)\n",
        "df.sort_index(ascending\u003dTrue, inplace\u003dTrue)\n",
        "df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "time_column.dtype"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpolation and smoothing \u003ca id\u003d\"interpolation\" /\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we can dive in further analysis, we need to make sure that we have a \"regular\" time series, ie that there are no holes or \"extraneous data\".\n",
        "\n",
        "We thus generate a \"resambled dataframe\".\n",
        "\n",
        "The first operation is to select the interpolation granularity, ie the time range that we\u0027ll standardize on. The following code automatically selects an appropriate granularity based on the total time range in your data:\n",
        "\n",
        "* Hour-based if data covers less than 1 month\n",
        "* Day-based if data covers less than 3 years\n",
        "* Week-based if data covers less than 10 years\n",
        "* Month-based if data covers less than 30 years\n",
        "* Year-based else"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "start_date \u003d df.index.min()\n",
        "end_date \u003d df.index.max()\n",
        "\n",
        "if (end_date - start_date) \u003c timedelta64(1, \u0027M\u0027):\n",
        "    sample_freq \u003d \"1H\"\n",
        "elif (end_date - start_date) \u003c timedelta64(3, \u0027Y\u0027):\n",
        "    sample_freq \u003d \"1D\"\n",
        "elif (end_date - start_date) \u003c timedelta64(10, \u0027Y\u0027):\n",
        "    sample_freq \u003d \"1W\"\n",
        "elif (end_date - start_date) \u003c timedelta64(30, \u0027Y\u0027):\n",
        "    sample_freq \u003d \"1M\"\n",
        "else:\n",
        "    sample_freq \u003d \"1A\"\n",
        "\n",
        "print \"Based on your data (timedelta: %s), chosen frequency is %s\" % ((end_date-start_date), sample_freq)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you\u0027d prefer to set your own sampling frequency, uncomment the following cell"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "# You can find the complete list of options at:\n",
        "# http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n",
        "#\n",
        "# You can combine these with numerical values. For example use \"2D\" for a \"every two days\" sample rate\n",
        "#\n",
        "#sample_freq \u003d \"1D\""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "df_interpolate \u003d df.resample(sample_freq, label\u003d\"left\", convention\u003d\"e\").mean()\n",
        "\n",
        "df_interpolate \u003d df_interpolate.interpolate(method\u003d\"time\")\n",
        "\n",
        "print \"Interpolated at %s, the time series has %s rows\" % (sample_freq, df_interpolate.shape[0])"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "Let\u0027s check how our resampled time series looks (NB: we only show the first time series here)"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Draw original and interpolated series\n",
        "fig, ax \u003d plt.subplots(1, figsize\u003d(17, 5))\n",
        "df[numerical_columns[0]].plot(alpha\u003d0.25)\n",
        "df_interpolate[numerical_columns[0]].plot()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting \u003ca id\u003d\"plots\" /\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### One above another, not scaled\n",
        "\n",
        "Let\u0027s plot all of our time series, one above another (NB: scales are not the same).\n",
        "\n",
        "Note that for display reason, we default to only plotting the first 20 time series. You can change this below."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": false
      },
      "source": [
        "# Plot all time series in this dataset, with a limit of 20\n",
        "lim_col \u003d min(20, len(numerical_columns))\n",
        "if lim_col \u003d\u003d 1:\n",
        "    f, ax \u003d plt.subplots(1, figsize\u003d(17, 3))\n",
        "    df_interpolate[numerical_columns[0]].plot()\n",
        "else:\n",
        "    f, ax \u003d plt.subplots(lim_col, figsize\u003d(17, 3 * lim_col))\n",
        "    for i, col in enumerate(numerical_columns[:20]):\n",
        "        ax[i].set_ylabel(col)\n",
        "        df_interpolate[col].plot(ax\u003dax[i])"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### On the same graph, not scaled\n",
        "\n",
        "Let\u0027s plot all of our time series on the same graph (with a single scale)"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "df_lim \u003d df_interpolate[numerical_columns[:lim_col]]\n",
        "df_lim.plot(use_index\u003dTrue, figsize\u003d(17,6))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### On the same graph, rescaled\n",
        "\n",
        "Let\u0027s plot all of our time series on the same graph after scaling all of them on the 0-1 range"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# The scaler doesn\u0027t accept NA values, so use filling and then drop all non-filled column\n",
        "df_nona \u003d df_interpolate.fillna(method\u003d\"ffill\")\n",
        "columns_to_drop \u003d []\n",
        "for col in df_nona.columns:\n",
        "    if df_nona[col].isnull().sum() \u003e 0:\n",
        "        columns_to_drop.append(col)\n",
        "df_nona \u003d df_nona.drop(columns_to_drop, axis\u003d1)\n",
        "plotted_columns \u003d list(set(numerical_columns[:lim_col]) - set(columns_to_drop))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "df_scaled \u003d pd.DataFrame(\n",
        "                    MinMaxScaler().fit_transform(df_nona[plotted_columns]),\n",
        "                    columns\u003dplotted_columns,\n",
        "                    index\u003ddf_nona.index)\n",
        "df_scaled.plot(use_index\u003dTrue, figsize\u003d(17, 6))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rolling windows (single time series) \u003ca id\u003d\"window\" /\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now going to plot the time series on various sizes of rolling windows. In essence, this is a low-pass filter on the data, which filters out high-frequency changes.\n",
        "\n",
        "For this whole section, we\u0027ll focus on a single time series. So let\u0027s start by selecting it"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "# We assume that the time series of interest is the first column of the dataframe.\n",
        "# You can replace \"numerical_columns[0]\" by your column of interest in the next line\n",
        "series_column \u003d numerical_columns[0]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "df_col \u003d df_interpolate[series_column]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "source": [
        " # Try to change these width with something smart for your data\n",
        "window_widths \u003d [5,20,50,100]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get mean value for varying window widths"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "f, ax \u003d plt.subplots(len(window_widths), figsize\u003d(17, 3 * len(window_widths)))\n",
        "for (i, window_size) in enumerate(window_widths):\n",
        "    ax[i].set_ylabel(\"Window size %s\" % window_size)\n",
        "    pd.Series.rolling(df_col, window\u003dwindow_size, center\u003dFalse).mean().plot(ax\u003dax[i])"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get min and max values for varying window widths"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "f, ax \u003d plt.subplots(len(window_widths), figsize\u003d(17, 3 * len(window_widths)))\n",
        "for (i, window_size) in enumerate(window_widths): # Try to change these width with something smart for your data\n",
        "    ax[i].set_ylabel(\"Window size %s\" % window_size)\n",
        "    pd.Series.rolling(df_col, window\u003dwindow_size, center\u003dFalse).min().plot(ax\u003dax[i], label\u003d\"Max\")\n",
        "    pd.Series.rolling(df_col, window\u003dwindow_size, center\u003dFalse).max().plot(ax\u003dax[i], label\u003d\"Min\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Min, mean, max on the same graph"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": false
      },
      "source": [
        "f, ax \u003d plt.subplots(len(window_widths), figsize\u003d(17, 3 * len(window_widths)))\n",
        "for (i, window_size) in enumerate(window_widths): # Try to change these width with something smart for your data\n",
        "    ax[i].set_ylabel(\"Window size %s\" % window_size)\n",
        "    \n",
        "    rolling \u003d  pd.Series.rolling(df_col, window\u003dwindow_size, center\u003dFalse)\n",
        "    mean \u003d rolling.mean()\n",
        "    mean.plot(ax\u003dax[i], label\u003d\"Mean\")\n",
        "    ax[i].fill_between(mean.index, rolling.min(), rolling.max(), alpha\u003d0.3)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STL Decomposition (single time series) \u003ca id\u003d\"seasonality\" /\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In many cases of time series analytics, there is a natural seasonal variation. \n",
        "\n",
        "A common analysis is to decompose the time series into the three main components:\n",
        "\n",
        "* Trend: the long-term evolution of the time series\n",
        "* Seasonality: the reproducible variation within each time period (as previously defined)\n",
        "* Random: unpredictable variations"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# The number of samples you want to use as timescale for the seasonality decomposition\n",
        "if sample_freq \u003d\u003d \"1W\":\n",
        "    freq \u003d 52\n",
        "elif sample_freq \u003d\u003d \"1M\":\n",
        "    freq \u003d 12\n",
        "elif sample_freq \u003d\u003d \"1D\":\n",
        "    freq \u003d 365\n",
        "else:\n",
        "    freq \u003d 52 # Change this with a seasonality you think should be present in your data.\n",
        "\n",
        "df_col_nona \u003d df_col.fillna(method\u003d\"ffill\")\n",
        "\n",
        "# Decompose time series\n",
        "decomposition \u003d seasonal_decompose(df_col_nona.values,freq\u003dfreq)\n",
        "trend \u003d decomposition.trend\n",
        "seasonal \u003d decomposition.seasonal\n",
        "residual \u003d decomposition.resid\n",
        "df_stl \u003d pd.DataFrame(index\u003ddf_col_nona.index)\n",
        "df_stl[\"Trend\"] \u003d trend\n",
        "df_stl[\"Seasonality\"] \u003d seasonal\n",
        "df_stl[\"Residuals\"] \u003d residual\n",
        "\n",
        "#Display    \n",
        "f, ax \u003d plt.subplots(3, figsize\u003d(15, 10))\n",
        "for (i, col) in enumerate(df_stl.columns):\n",
        "    ax[i].set_ylabel(col)\n",
        "    df_stl[col].plot(ax\u003dax[i])"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Signal processing \u003ca id\u003d\"signal\" /\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rest of the analysis is dedicated to \"signal processing\" kind of analysis and is generally not suitable for general-purpose time series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Finding the peaks\n",
        "\n",
        "This code finds the peaks in the input signal, and plots the detected peaks together with the original data"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "from scipy.signal import find_peaks_cwt\n",
        "ts_fill \u003d df_col.fillna(method\u003d\u0027ffill\u0027).fillna(method\u003d\u0027bfill\u0027)  \n",
        "max_octave \u003d np.log2(ts_fill.shape[0])\n",
        "widths \u003d 2**np.arange(0, max_octave)\n",
        "peaks \u003d find_peaks_cwt(ts_fill, widths) # list\n",
        "ts_peaks \u003d pd.Series(0, index\u003dts_fill.index)\n",
        "ts_peaks.iloc[peaks] \u003d 1"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "fig, ax \u003d plt.subplots(1, figsize\u003d(17, 3))\n",
        "\n",
        "ax.plot(df_col.index, df_col, label\u003d\u0027Data\u0027, color\u003d\u0027blue\u0027)\n",
        "ax.plot(ts_peaks.index, ts_peaks * df_col.max(), label\u003d\u0027Peaks\u0027, color\u003d\u0027red\u0027)\n",
        "ax.set_title(\u0027Peaks\u0027)\n",
        "ax.legend(loc\u003d\"upper left\")\n",
        "ax.grid()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Power spectrum estimation"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "time_range \u003d df_interpolate.index.max() - df_interpolate.index.min()\n",
        "sampling_frequency \u003d df_interpolate.index.shape[0]/ (time_range.total_seconds())\n",
        "print \"Time series covers %s seconds (sampling freq %s Hz, sampling period: %ss)\" % \\\n",
        "                    (time_range.total_seconds(), sampling_frequency, 1/sampling_frequency)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "from scipy import signal\n",
        "d1, d2 \u003d signal.welch(df_col.fillna(method\u003d\"ffill\"), sampling_frequency)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "plt.semilogy(d1, d2)\n",
        "plt.xlabel(\u0027Frequency [Hz]\u0027)\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Continuous Wavelet transform\n",
        "\n",
        "NB: For best results, don\u0027t interpolate too much for this."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "from scipy import signal\n",
        "widths \u003d np.arange(1, 10)\n",
        "\n",
        "outputs \u003d [\"wavelet_width%i\" % (int(w)) for w in widths]\n",
        "\n",
        "df_col_filled \u003d df_col.fillna(method\u003d\"ffill\")\n",
        "\n",
        "cwtmat \u003d signal.cwt(df_col_filled.values, signal.ricker, widths)\n",
        "cwtmat_df \u003d pd.DataFrame(cwtmat.T, columns\u003doutputs, index\u003ddf_col.index)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "fig, ax \u003d plt.subplots(2, figsize\u003d(17, 6))\n",
        "ax[0].plot(df_col_filled.index, df_col_filled.values)\n",
        "ax[1].imshow(cwtmat, aspect\u003d\"auto\", cmap\u003d\u0027PRGn\u0027)"
      ],
      "outputs": []
    }
  ]
}